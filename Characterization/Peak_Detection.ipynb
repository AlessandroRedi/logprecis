{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e71dab-c378-4eb0-a650-80a17555c1f6",
   "metadata": {},
   "source": [
    "# This notebook wants to prove the usefuleness of the model on decreasing the experts analysis from thousands of unique sessions a day to tens of unique intents.\n",
    "## Idea here is to provide the security expert a tool to easily focus on \"real\" novelties\n",
    "### Furthermore, we also want to analyse the relationship between new sessions (according to our labels) and new labels per day "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7077b3-4197-4a52-b4e1-1c4c70b0558f",
   "metadata": {},
   "source": [
    "### Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f850641-fe5a-40a5-ba6a-a399fc9a34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f6be8-e981-40d8-a4d9-bdec603bbf18",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826285b-f735-4b1f-959a-fe28e6f14301",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_corpus = pd.read_csv(f\"../Inference/corpus_with_predictions.csv\")\n",
    "predicted_corpus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee7089-db6f-4d59-a26e-f865bd8998f9",
   "metadata": {},
   "source": [
    "#### How many unique sessions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1f92f0-88fb-46f8-aefb-dd5e21bfdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique sessions: {predicted_corpus.full_session.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884e141-3de3-49d0-836e-8cd7222b9f47",
   "metadata": {},
   "source": [
    "#### How many unique predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0c0a4-72b8-4bad-b8dd-0e0b4e1096a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique predictions: {predicted_corpus.Models_predictions.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f32827-bdbb-4795-a527-db09b82584c1",
   "metadata": {},
   "source": [
    "##### MINOR: Remember to cast \"first_timestamp\" string to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda0f0f-401f-441c-af9f-93d3756de832",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_corpus[\"first_timestamp\"] = pd.to_datetime(predicted_corpus[\"first_timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3602142-6752-403a-87d2-1160ff73c32e",
   "metadata": {},
   "source": [
    "#### Make sure we are handling only unique sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5dfe9-2c6c-4997-a7c1-27453f0d2110",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before dropping duplicates: {predicted_corpus.shape[0]}\")\n",
    "predicted_corpus.sort_values(by = \"first_timestamp\", ascending = True, inplace = True)\n",
    "predicted_corpus = predicted_corpus.drop_duplicates([\"full_session\"])\n",
    "print(f\"After dropping duplicates: {predicted_corpus.shape[0]}\")\n",
    "predicted_corpus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cecfd6-bab7-4c4e-afcd-adc8506973f0",
   "metadata": {},
   "source": [
    "#### How long did the collection last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b5b45-09c1-4b4b-9928-68d89a9ea3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = predicted_corpus.first_timestamp.min()\n",
    "stop = predicted_corpus.first_timestamp.max()\n",
    "print(f\"Collection started in {start} and lasted untill {stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af6e87-0044-485e-ab6b-c0a4ee97a2fe",
   "metadata": {},
   "source": [
    "#### Create \"date\" feature to aggregate daily stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbba9fd-46cf-43fe-b122-71814eb7a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_corpus[\"date\"] = predicted_corpus[\"first_timestamp\"].progress_apply(lambda datetime: datetime.date())\n",
    "predicted_corpus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010e6f4-55dd-47bb-804a-ca758dedda02",
   "metadata": {},
   "source": [
    "#### Which distribution of unique sessions/day? Group daily stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee96f9-3b5b-4309-aa6c-5aaa3136a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby date and set date as an index\n",
    "unique_session_per_date = predicted_corpus.groupby(\"date\")[\"full_session\"].count().reset_index().rename({\"full_session\":\"unique_sessions_per_day\"}, axis = 1).sort_values(by = \"date\")\n",
    "unique_session_per_date.set_index(unique_session_per_date.date, inplace = True)\n",
    "unique_session_per_date.drop(\"date\", axis = 1, inplace = True)\n",
    "# Now refill dates in which the honeypot was off with None\n",
    "idx = pd.date_range(predicted_corpus.date.min(), predicted_corpus.date.max())\n",
    "unique_session_per_date = unique_session_per_date.reindex(idx, fill_value=None)\n",
    "unique_session_per_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b040bd44-b84b-4b93-a91c-b8379cb8cf92",
   "metadata": {},
   "source": [
    "#### Now, find the number of unique predictions/day according to the model\n",
    "##### Ide behind unique predictions is that we remove duplicates PER DAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac4b8d-0031-45d7-a967-53ec3c2865c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby date and set date as an index\n",
    "unique_labels_per_date = predicted_corpus.groupby(\"date\")[\"Models_predictions\"].nunique().reset_index().rename({\"Models_predictions\":\"unique_labels_per_day\"}, axis = 1).sort_values(by = \"date\")\n",
    "unique_labels_per_date.set_index(unique_labels_per_date.date, inplace = True)\n",
    "unique_labels_per_date.drop(\"date\", axis = 1, inplace = True)\n",
    "# Now refill dates in which the honeypot was off with None\n",
    "idx = pd.date_range(predicted_corpus.date.min(), predicted_corpus.date.max())\n",
    "unique_labels_per_date = unique_labels_per_date.reindex(idx, fill_value=None)\n",
    "unique_labels_per_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6126b1a-96f2-4737-8f48-d88239efaa89",
   "metadata": {},
   "source": [
    "#### Plot the two trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfa9dd-2191-4fba-9a46-66658b14b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "fontsize = 15\n",
    "\n",
    "# First ax\n",
    "ax1.plot(unique_session_per_date.index, unique_session_per_date.unique_sessions_per_day, linewidth = 2, color = \"royalblue\", label = \"|Unique sessions|\")\n",
    "ax1.set_ylabel('|Unique sessions|', fontsize = fontsize + 3)\n",
    "ax1.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax1.set_xlabel('Date', fontsize = fontsize + 3)\n",
    "ax1.xaxis.set_tick_params(labelsize=fontsize, rotation = 60)  \n",
    "ax1.grid(linewidth = .5)\n",
    "\n",
    "# Second ax\n",
    "ax2.plot(unique_labels_per_date.index, unique_labels_per_date.unique_labels_per_day, linewidth = 2, color = \"firebrick\", label = \"|Unique labels|\", alpha = .7)\n",
    "ax2.set_ylabel('|Unique labels|', fontsize = fontsize + 3)\n",
    "ax2.set_xlabel('Date', fontsize = fontsize + 3)\n",
    "ax2.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax2.xaxis.set_tick_params(labelsize=fontsize, rotation = 60)   \n",
    "ax2.grid(linewidth = .5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9682482-9e04-40de-b943-c8afe63b0132",
   "metadata": {},
   "source": [
    "**COMMENT**: The plot above shows that we indeed perform an aggregation, moving from ~ 500 unique sessions per day to ~ 30 unique labels per day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d654f1c2-d882-45b4-8127-5a3b97c01bf2",
   "metadata": {},
   "source": [
    "### Now, which is the relationship between #unique sessions per day and #new sessions per day? And which is the one between #unique labels and #new labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1768b-2fe5-4555-b6b4-c6fef2e31b1c",
   "metadata": {},
   "source": [
    "#### For each label, getting the date of first appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c27c5b-a910-499f-8450-ae85525a05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_first_appearances = predicted_corpus[[\"date\", \"Models_predictions\"]].sort_values(by = \"date\").drop_duplicates([\"Models_predictions\"])\n",
    "labels_first_appearances = labels_first_appearances.rename({\"date\":\"first_appearance\"}, axis = 1)\n",
    "labels_first_appearances.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c9e76-6f0b-4f4a-914e-c74d4df2e369",
   "metadata": {},
   "source": [
    "#### Now, obtain new sessions per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5f4b7-37ae-4714-9b29-cc0ab37fc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before: {predicted_corpus.shape[0]}\")\n",
    "joined_corpus = predicted_corpus.merge(labels_first_appearances, on = \"Models_predictions\")\n",
    "joined_corpus = joined_corpus[joined_corpus.date == joined_corpus.first_appearance]\n",
    "print(f\"After: {joined_corpus.shape[0]}\")\n",
    "joined_corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df40d78-dd31-4503-813c-8373941406fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby date and set date as an index\n",
    "new_sessions_per_date = joined_corpus.groupby(\"date\")[\"full_session\"].count().reset_index().rename({\"full_session\":\"new_sessions_per_day\"}, axis = 1).sort_values(by = \"date\")\n",
    "new_sessions_per_date.set_index(new_sessions_per_date.date, inplace = True)\n",
    "new_sessions_per_date.drop(\"date\", axis = 1, inplace = True)\n",
    "# Now refill dates in which the honeypot was off with None\n",
    "idx = pd.date_range(predicted_corpus.date.min(), predicted_corpus.date.max())\n",
    "new_sessions_per_date = new_sessions_per_date.reindex(idx, fill_value=None)\n",
    "new_sessions_per_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93748271-1a20-44a5-b11f-7a811f490ac0",
   "metadata": {},
   "source": [
    "#### Also, obtain the number of new labels per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bdf30-1e69-4fd2-aac5-b67a19a2608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels_per_date = joined_corpus.groupby(\"date\")[\"Models_predictions\"].nunique().reset_index().rename({\"Models_predictions\":\"new_labels_per_day\"}, axis = 1).sort_values(by = \"date\")\n",
    "new_labels_per_date.set_index(new_labels_per_date.date, inplace = True)\n",
    "new_labels_per_date.drop(\"date\", axis = 1, inplace = True)\n",
    "# Now refill dates in which the honeypot was off with None\n",
    "idx = pd.date_range(predicted_corpus.date.min(), predicted_corpus.date.max())\n",
    "new_labels_per_date = new_labels_per_date.reindex(idx, fill_value=None)\n",
    "new_labels_per_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4282dc-e41e-48d6-851f-76ec39e0cfa1",
   "metadata": {},
   "source": [
    "#### Only New Predictions per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552e80c-72d7-4b3a-8f68-9ca495984c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "server_update_date = datetime.strptime(\"08/11/2019\", '%d/%m/%Y').date()\n",
    "server_update_datetime = datetime.strptime(\"08/11/2019\", '%d/%m/%Y')\n",
    "\n",
    "print(f\"Server update occurred in {server_update_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3563168-adc7-41d2-9bd6-2c5801297b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, figsize=(6,5))\n",
    "fontsize = 17\n",
    "\n",
    "ax1.plot(unique_session_per_date.index, unique_session_per_date.unique_sessions_per_day, linewidth = 2, color = \"royalblue\", label = \"|Unique sessions|\")\n",
    "ax1.vlines(server_update_datetime, 0, 6500, label = \"Server update\", linewidth = 1.5, linestyle = \"dashed\", color = \"firebrick\")\n",
    "\n",
    "ax1.set_ylabel('|Unique sessions|', fontsize = fontsize + 2)\n",
    "ax1.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax1.set_xlabel('Date', fontsize = fontsize + 2)\n",
    "ax1.xaxis.set_tick_params(labelsize=fontsize, rotation = 30)  \n",
    "ax1.grid(linewidth = .5)\n",
    "\n",
    "\n",
    "server_update = Line2D([0], [0], color='firebrick', linestyle = \"dashed\", label= \"Server update\")\n",
    "handles = [server_update]\n",
    "ax1.legend(handles=handles, fontsize = fontsize, loc = \"upper left\", framealpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Inference_results/unique_sessions_in_time.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46258623-7089-405c-8d87-5627cb11d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, figsize=(6,5))\n",
    "fontsize = 17\n",
    "\n",
    "# Second ax\n",
    "ax1.plot(new_labels_per_date.index, new_labels_per_date.new_labels_per_day, linewidth = 2, color = \"forestgreen\")\n",
    "ax1.vlines(server_update_datetime, 0, 50, label = \"Server update\", linewidth = 1.5, linestyle = \"dashed\", color = \"firebrick\")\n",
    "\n",
    "ax1.set_ylabel('|New tactical fingerprints|', fontsize = fontsize + 2)\n",
    "ax1.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax1.xaxis.set_tick_params(labelsize=fontsize, rotation = 30)\n",
    "ax1.set_yticks(np.arange(0, 51, 10))\n",
    "ax1.grid()\n",
    "ax1.set_ylim(0, 50)\n",
    "ax1.set_xlabel(\"Date\", fontsize = fontsize + 2)\n",
    "\n",
    "server_update = Line2D([0], [0], color='firebrick', linestyle = \"dashed\", label= \"Server update\")\n",
    "handles = [server_update]\n",
    "ax1.legend(handles=handles, fontsize = fontsize, loc = \"upper left\", framealpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./Inference_results/novelties_in_time.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77434d09-f62e-4f36-aa4a-a28aff93cc01",
   "metadata": {},
   "source": [
    "#### Other version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108083d3-c85a-43ea-a395-766d3851b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,6))\n",
    "fontsize = 20\n",
    "\n",
    "# First ax\n",
    "ax1.plot(unique_session_per_date.index, unique_session_per_date.unique_sessions_per_day, linewidth = 2, color = \"royalblue\", label = \"|Unique sessions|\")\n",
    "ax1.set_ylabel('|Unique sessions|', fontsize = fontsize + 3)\n",
    "ax1.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax1.set_xlabel('Date', fontsize = fontsize + 3)\n",
    "ax1.xaxis.set_tick_params(labelsize=fontsize, rotation = 60)  \n",
    "ax1.grid(linewidth = .5)\n",
    "\n",
    "# Second ax\n",
    "ax2.plot(new_labels_per_date.index, new_labels_per_date.new_labels_per_day, linewidth = 2, color = \"darkred\")\n",
    "ax2.set_ylabel('|New Labels|', fontsize = fontsize + 3)\n",
    "ax2.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax2.xaxis.set_tick_params(labelsize=fontsize, rotation = 60)      \n",
    "ax2.grid()\n",
    "ax2.set_xlabel('Date', fontsize = fontsize + 3)\n",
    "\n",
    "plt.tight_layout(pad=5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c1f96-2634-49a1-abbe-0f3e107618c6",
   "metadata": {},
   "source": [
    "#### Isolate peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a65b5-2398-4fe7-8cd2-59a55adbe5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "desired_date = date(2019, 11, 12)\n",
    "novelties = labels_first_appearances[labels_first_appearances.first_appearance == desired_date]\n",
    "print(f\"Selected {novelties.shape[0]} sequences of predictions\")\n",
    "novelties.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cf149-9b39-4a79-a963-85acc9770b58",
   "metadata": {},
   "source": [
    "##### How many associated sessions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952acaf-1c4e-4f81-9bc7-6e10f43fba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "novelties = novelties.merge(joined_corpus.groupby(\"Models_predictions\")[\"full_session\"].count().reset_index(name = \"associated_sessions\"), on = \"Models_predictions\")\n",
    "novelties.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff5531-7e7f-48d7-ac59-1d87f4fd2d82",
   "metadata": {},
   "source": [
    "##### How different/\"distant\" are those novelties in terms of edit distances (weighted Levenstein distance)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abba839-5d6a-4a60-8fa8-22ed5593bdbf",
   "metadata": {},
   "source": [
    "###### Create OneHotEncoded versions of inputs so that we can compute word level levenstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470ec31-c845-4ce4-a7a9-07052f3f5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Dataset/Training/Supervised/labels.txt\", \"r\") as f:\n",
    "    labels = [el.strip() for el in f.readlines()]\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3d793-e810-416c-85be-0e073041123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "novelties[\"oneHotEncoded\"] = novelties[\"Models_predictions\"].apply(lambda prediction: \"\".join([str(label2id[el]) for el in prediction.split(\" -- \")]))\n",
    "novelties.sort_values(by = \"associated_sessions\", ascending = False, inplace = True)\n",
    "novelties.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd69ba8-b5f7-4af7-acc7-2cb61235a779",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How \"distant\" are the attacks of interest from each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abf684-40ab-4e07-87a0-de5c6427c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as lev\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "\n",
    "perms_values = permutations(list(novelties.oneHotEncoded.values), 2)\n",
    "perms_keys = permutations(list(novelties.Models_predictions.values), 2)\n",
    "\n",
    "id2sequence = dict(zip(list(np.arange(novelties.shape[0])), list(novelties.Models_predictions.values)))\n",
    "sequence2id = dict(zip(list(novelties.Models_predictions.values), list(np.arange(novelties.shape[0]))))\n",
    "\n",
    "distances = []\n",
    "for key, value in zip(perms_keys, perms_values):\n",
    "    distance = lev(value[0], value[1])\n",
    "    max_length = np.max([len(key[1].split(\" -- \")), len(key[0].split(\" -- \"))])\n",
    "    distances.append({\n",
    "        \"origin\":sequence2id[key[0]],\n",
    "        \"destination\": sequence2id[key[1]],\n",
    "        \"distance\": distance/max_length\n",
    "    })\n",
    "    \n",
    "df_tmp = pd.DataFrame(distances)\n",
    "\n",
    "table = pd.pivot_table(df_tmp, values='distance', index=['origin'], columns = [\"destination\"], aggfunc=np.sum).fillna(0)\n",
    "\n",
    "mask = np.zeros_like(table.to_numpy(), dtype=\"bool\")\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "fig, axs = plt.subplots(figsize =(10, 6))\n",
    "fontsize = 15\n",
    "\n",
    "axs = sns.heatmap(table.to_numpy(), mask=mask, linewidth = 0.2 ,cmap=\"jet\")\n",
    "\n",
    "cbar = axs.collections[0].colorbar\n",
    "# here set the labelsize by 20\n",
    "cbar.ax.tick_params(labelsize=fontsize)\n",
    "\n",
    "axs.figure.axes[-1].yaxis.label.set_size(fontsize+5)\n",
    "axs.figure.axes[-1].yaxis.set_label_coords(3,.5)\n",
    "axs.set_ylabel(\"Origin\", fontsize = fontsize + 5)\n",
    "axs.set_xlabel(\"Destination\", fontsize = fontsize + 5)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f46cfd-e7b6-496d-b76a-368ba7c39a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in [0, 4, 5, 8, 12, 14, 29, 31]:\n",
    "    prediction = novelties.iloc[it].Models_predictions\n",
    "    print(it)\n",
    "    print(joined_corpus[joined_corpus.Models_predictions == prediction].iloc[0].Models_predictions)\n",
    "    print()\n",
    "    print(joined_corpus[joined_corpus.Models_predictions == prediction].iloc[0].full_session)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a37e7-5b75-411d-9b74-8166403f01c5",
   "metadata": {},
   "source": [
    "#### Focus on one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f64b17-011b-41ea-96fd-56db13f3cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_family = novelties.iloc[0].Models_predictions\n",
    "print(f\"Focusing on:\\n{chosen_family}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb801f0-61a8-4804-9b6e-04487221e9a2",
   "metadata": {},
   "source": [
    "##### Let's try plotting the novelty of the predictions vs days passed from closest neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3875d-a8f1-4ded-8732-9ad2246d5d43",
   "metadata": {},
   "source": [
    "###### For each label, save first date in which we've seen that label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47395cff-6772-46b8-8452-58da7e22bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_appearance_per_prediction = predicted_corpus.groupby(\"Models_predictions\")[\"date\"].agg(\"first\").reset_index()\n",
    "first_appearance_per_prediction.sort_values(by = \"date\", ascending = True, inplace = True)\n",
    "first_appearance_per_prediction.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70c617-ccda-4171-b2fc-c37f00d1b934",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Compute distances between families "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874cc2f-a404-483b-9906-972298e92a1b",
   "metadata": {},
   "source": [
    "###### Create OneHotEncoded versions of inputs so that we can compute word level levenstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36632bac-101f-4e70-8196-2b4dcc7a6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_appearance_per_prediction[\"OneHotEncoded_representation\"] = first_appearance_per_prediction[\"Models_predictions\"].apply(lambda prediction: \"\".join([str(label2id[el]) for el in prediction.split(\" -- \")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16621d-7e86-49a1-84e6-57fbc7f8a6e4",
   "metadata": {},
   "source": [
    "###### Compute distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e030e-2422-4672-9459-7ca1445d12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib import tzip\n",
    "from itertools import permutations\n",
    "from Levenshtein import distance as lev\n",
    "import numpy as np\n",
    "\n",
    "perms_values = list(permutations(list(first_appearance_per_prediction.OneHotEncoded_representation.values), 2))\n",
    "perms_keys = list(permutations(list(first_appearance_per_prediction.Models_predictions.values), 2))\n",
    "\n",
    "distances = []\n",
    "for it in tqdm(range(len(list(perms_keys)))):\n",
    "    key, value = perms_keys[it], perms_values[it]\n",
    "    distance = lev(value[0], value[1])\n",
    "    max_length = np.max([len(key[1].split(\" -- \")), len(key[0].split(\" -- \"))])\n",
    "    distances.append({\n",
    "        \"origin\":key[0],\n",
    "        \"destination\": key[1],\n",
    "        \"distance\": distance#/max_length\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb513a-fc41-4406-b0ec-d7a21b8eb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_df = pd.DataFrame(distances)\n",
    "distances_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7510790-1aab-41e3-82bf-78f15d92b17c",
   "metadata": {},
   "source": [
    "###### Create OD matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a97ac1-3fd6-420f-9bb1-910d186ef610",
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_matrix = distances_df.pivot_table(values='destination', index=\"origin\", columns='destination', aggfunc=sum)\n",
    "print(f\"Created OD matrix of size {OD_matrix.shape[0]}x{OD_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c4820-bb69-473f-bd08-44afa309fe2f",
   "metadata": {},
   "source": [
    "##### Find index of chosen family + date of birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e764e-32cd-44ac-8716-3e0e804641ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev_next_origins = [first_appearance_per_prediction[first_appearance_per_prediction.Models_predictions == chosen_family].index[0]][0]\n",
    "origin_representation = first_appearance_per_prediction.loc[prev_next_origins][\"date\"]\n",
    "origin_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b4fb0-10cd-4333-8956-a41106157147",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "to_beginning_of_dataset = False\n",
    "beginnin_of_dataset = predicted_corpus.date.min()\n",
    "hop = 0\n",
    "top_neigh = 1\n",
    "selected_date = desired_date\n",
    "\n",
    "weighted_edges = []\n",
    "prev_next_origins = [first_appearance_per_prediction[first_appearance_per_prediction.Models_predictions == chosen_family].index[0]]\n",
    "while not to_beginning_of_dataset:\n",
    "    print(f\"\\nHop: {hop + 1} -->\", end = \"\\t\")\n",
    "    print(f\"At this level, {len(prev_next_origins)} origins...\", end = \" \")\n",
    "    next_origins = []\n",
    "    for origin_id in prev_next_origins:\n",
    "        origin_prediction = first_appearance_per_prediction.loc[origin_id][\"Models_predictions\"]\n",
    "        origin_date = first_appearance_per_prediction.loc[origin_id][\"date\"]\n",
    "        # From OD matrix, find possible destinations given origin\n",
    "        all_destinations = OD_matrix.loc[origin_prediction]\n",
    "        families_before_selected = first_appearance_per_prediction[first_appearance_per_prediction.date < origin_date].Models_predictions\n",
    "        possible_destinations = all_destinations[families_before_selected]\n",
    "        sorted_possible_destinations = possible_destinations.reset_index().sort_values(by = origin_prediction)\n",
    "        for it in range(top_neigh):\n",
    "            destination = sorted_possible_destinations.iloc[it][\"destination\"]\n",
    "            distance_destination = sorted_possible_destinations.iloc[it][origin_prediction]\n",
    "            id_destination = first_appearance_per_prediction[first_appearance_per_prediction.Models_predictions == destination].index[0]\n",
    "            date_destination = first_appearance_per_prediction.loc[id_destination].date\n",
    "            if date_destination == beginnin_of_dataset: #Stopping condition\n",
    "                to_beginning_of_dataset = True\n",
    "            weighted_edges.append((int(origin_id), int(id_destination), distance_destination, len(origin_prediction.split(\" -- \")), origin_date))\n",
    "            next_origins.append(id_destination)\n",
    "    prev_next_origins = [el for el in next_origins]\n",
    "    print(f\"And {len(prev_next_origins)} destinations!\")\n",
    "    hop += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb79e33-4dcc-4a0d-8af5-7338ca151ed1",
   "metadata": {},
   "source": [
    "#### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900e1c4-ece2-44b4-b666-d1e6f7c2dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.DataFrame(weighted_edges, columns = [\"Origin\", \"Destination\", \"edit_distance\", \"|words_origin|\", \"day_of_novelty\"])\n",
    "edges_df[\"ordered_OD\"] = edges_df.apply(lambda row: \" - \".join(sorted([str(row[\"Origin\"]), str(row[\"Destination\"])])), axis = 1)\n",
    "print(f\"Final graph contains {edges_df['ordered_OD'].nunique()} edges\")\n",
    "edges_df[[\"Origin\", \"Destination\", \"edit_distance\", \"|words_origin|\", \"day_of_novelty\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14079ae1-bf1a-4cf1-b53a-879a0d2c2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from datetime import timedelta\n",
    "\n",
    "fig, (ax) = plt.subplots(1, figsize=(8,6))\n",
    "fontsize = 15\n",
    "edges_df.sort_values(by = \"day_of_novelty\", inplace = True)\n",
    "\n",
    "#Colorbar\n",
    "norm = plt.Normalize(edges_df['|words_origin|'].min(), edges_df['|words_origin|'].max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"flare\", norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "# First ax\n",
    "ax.plot(edges_df.day_of_novelty, edges_df.edit_distance, linewidth = .5, color = \"navy\")\n",
    "sns.scatterplot(data=edges_df, x=\"day_of_novelty\", y =\"edit_distance\", hue = \"|words_origin|\", palette = \"flare\", marker='D', ax = ax)\n",
    "ax.collections[0].set_sizes([200])                 # <---- reset markersize here\n",
    "ax.set_ylabel('|Words edited|', fontsize = fontsize + 3)\n",
    "ax.yaxis.set_tick_params(labelsize=fontsize)\n",
    "ax.set_xlabel('Date', fontsize = fontsize + 3)\n",
    "ax.xaxis.set_tick_params(labelsize=fontsize, rotation = 60)  \n",
    "ax.grid(linewidth = .5)\n",
    "\n",
    "# Remove the legend and add a colorbar\n",
    "ax.get_legend().remove()\n",
    "cbar = ax.figure.colorbar(sm)\n",
    "cbar.ax.get_yaxis().labelpad = 20\n",
    "cbar.set_label('|words per sequence|', rotation=270, fontsize=fontsize + 2)\n",
    "\n",
    "\n",
    "#Text\n",
    "for i in range(edges_df.shape[0]):\n",
    "    txt = edges_df.iloc[i].Origin\n",
    "    x, y = edges_df.iloc[i].day_of_novelty, edges_df.iloc[i].edit_distance\n",
    "    t = ax.annotate(txt, (x + timedelta(days=5), y - 2), fontsize = fontsize)\n",
    "    t.set_bbox(dict(facecolor='red', alpha=0.2, edgecolor='salmon'))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e11f75-9ad4-43e2-818b-c9705e057741",
   "metadata": {},
   "source": [
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573b35d-4ac6-4fde-be38-3865650aedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repetitions(sequence_intents):\n",
    "    list_elements = sequence_intents.split(\" -- \")\n",
    "    prev_el = list_elements[0]\n",
    "    non_repeated_list = []\n",
    "    counter = 1\n",
    "    for it in range(1,len(list_elements)):\n",
    "        el = list_elements[it]\n",
    "        if prev_el != el:\n",
    "            non_repeated_list.append(f\"{prev_el} x {counter}\")\n",
    "            counter = 1\n",
    "            prev_el = el\n",
    "        else:\n",
    "            counter += 1\n",
    "    # For last element\n",
    "    non_repeated_list.append(f\"{prev_el} x {counter}\")        \n",
    "    return \" -- \".join(non_repeated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952ec6e-7627-4807-9453-d702868ccb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node 227:\\nFirst appearance:\\t{first_appearance_per_prediction.loc[227].date}\\nCorresponding intent:\\t{remove_repetitions(first_appearance_per_prediction.loc[227].Models_predictions)}\")\n",
    "print(f\"\\nExample: {predicted_corpus[predicted_corpus.Models_predictions == first_appearance_per_prediction.loc[227].Models_predictions].full_session.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e0dc6-c3fd-4581-9046-ca1c64e1463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node 101:\\nFirst appearance:\\t{first_appearance_per_prediction.loc[101].date}\\nCorresponding intent:\\t{remove_repetitions(first_appearance_per_prediction.loc[101].Models_predictions)}\")\n",
    "print(f\"\\nExample: {predicted_corpus[predicted_corpus.Models_predictions == first_appearance_per_prediction.loc[101].Models_predictions].full_session.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db23538-1924-4de1-9aae-3126ea4bea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node 192:\\nFirst appearance:\\t{first_appearance_per_prediction.loc[192].date}\\nCorresponding intent:\\t{remove_repetitions(first_appearance_per_prediction.loc[192].Models_predictions)}\")\n",
    "print(f\"\\nExample: {predicted_corpus[predicted_corpus.Models_predictions == first_appearance_per_prediction.loc[192].Models_predictions].full_session.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231a1e1-12db-4854-bca4-4eab530101b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node 1591:\\nFirst appearance:\\t{first_appearance_per_prediction.loc[1591].date}\\nCorresponding intent:\\t{remove_repetitions(first_appearance_per_prediction.loc[1591].Models_predictions)}\")\n",
    "print(f\"\\nExample: {predicted_corpus[predicted_corpus.Models_predictions == first_appearance_per_prediction.loc[1591].Models_predictions].full_session.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144d38e-a35f-44bd-bdad-f6110998fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node 1384:\\nFirst appearance:\\t{first_appearance_per_prediction.loc[1384].date}\\nCorresponding intent:\\t{remove_repetitions(first_appearance_per_prediction.loc[1384].Models_predictions)}\")\n",
    "print(f\"\\nExample: {predicted_corpus[predicted_corpus.Models_predictions == first_appearance_per_prediction.loc[1384].Models_predictions].full_session.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7291f0-657f-482e-98cc-c964a8fc5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node 76:\\nFirst appearance:\\t{first_appearance_per_prediction.loc[76].date}\\nCorresponding intent:\\t{remove_repetitions(first_appearance_per_prediction.loc[76].Models_predictions)}\")\n",
    "print(f\"\\nExample: {predicted_corpus[predicted_corpus.Models_predictions == first_appearance_per_prediction.loc[76].Models_predictions].full_session.iloc[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
