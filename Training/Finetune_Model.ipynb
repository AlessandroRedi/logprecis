{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06aef61-5584-4565-a8a2-4898cbefcd7a",
   "metadata": {},
   "source": [
    "# This notebook solves a problem of Token Classification\n",
    "## Referring to: https://huggingface.co/course/chapter7/2?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f7520-381b-45ce-b1bf-7edaadaceb55",
   "metadata": {},
   "source": [
    "### Now, use the hyper-parameters from the seeds before to train the \"best\" model \n",
    "#### For this model, we will use only a training set (stopping condition will be average stopping epoch from 5 seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b85371-4aca-4c05-88d4-473aa3ff5f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model will keep training up to epoch: 47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "stopping_epochs = 47 #Empirically found with grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc156684-24d9-4f98-a012-beb567b3b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid warnings\n",
    "import warnings\n",
    "import datasets\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "datasets.utils.logging.set_verbosity(datasets.utils.logging.ERROR)\n",
    "datasets.utils.logging.enable_progress_bar()\n",
    "\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity(transformers.utils.logging.ERROR)\n",
    "transformers.utils.logging.enable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c70145-53a2-475f-9e46-95eccce1af91",
   "metadata": {},
   "source": [
    "### First import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360596cd-4f09-4666-b252-a93a4d01860b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_session</th>\n",
       "      <th>tokens_labels</th>\n",
       "      <th>session_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>indexes_statements_context</th>\n",
       "      <th>indexes_words_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['which', 'awk', ';', 'echo', '6z18a0jzqrz1', ...</td>\n",
       "      <td>['Discovery', 'Discovery', 'Discovery', 'Disco...</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['ps', '-x', ';', 'ps', '-x', ';', 'cat', '/pr...</td>\n",
       "      <td>['Discovery', 'Discovery', 'Discovery', 'Disco...</td>\n",
       "      <td>188</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tokenized_session  \\\n",
       "0  ['which', 'awk', ';', 'echo', '6z18a0jzqrz1', ...   \n",
       "1  ['ps', '-x', ';', 'ps', '-x', ';', 'cat', '/pr...   \n",
       "\n",
       "                                       tokens_labels  session_id  order_id  \\\n",
       "0  ['Discovery', 'Discovery', 'Discovery', 'Disco...         135         1   \n",
       "1  ['Discovery', 'Discovery', 'Discovery', 'Disco...         188         1   \n",
       "\n",
       "  indexes_statements_context indexes_words_context  \n",
       "0                         []                    []  \n",
       "1                         []                    []  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "PATH = \"../Dataset/Training/Supervised/\"\n",
    "dataset = pd.read_csv(f\"{PATH}token_classification.csv\")\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c14768-55b0-4501-aa6d-7e56d1f548cf",
   "metadata": {},
   "source": [
    "#### Shuffle the dataset, but use only ONE partition (training)\n",
    "##### We will use the session_ids here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53b81038-e888-4ff5-9008-a5684d133592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 359 (entire dataset)\n"
     ]
    }
   ],
   "source": [
    "seed = 1 # Seed used to shuffle dataset\n",
    "shuffled_indexes = dataset.session_id.drop_duplicates().sample(frac = 1, random_state = seed)\n",
    "print(f\"Training set: {shuffled_indexes.shape[0]} (entire dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc87af-16fc-4493-aa01-84931325bda0",
   "metadata": {},
   "source": [
    "#### Create partitions\n",
    "##### Back to the original sessions (keeping subsessions with the same session_id together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2293ab5f-963c-4102-ab1a-64e72c4a8221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training df: 597\n"
     ]
    }
   ],
   "source": [
    "train_df = dataset[dataset['session_id'].isin(shuffled_indexes)]\n",
    "print(f\"Training df: {train_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e8bcd-1a4b-4504-934b-fbd6245ff703",
   "metadata": {},
   "source": [
    "#### Now create huggingface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c73de71-303b-46d9-a27e-92969f02ff42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokenized_session', 'tokens_labels', 'session_id', 'order_id', 'indexes_statements_context', 'indexes_words_context'],\n",
       "    num_rows: 597\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "labeled_dataset = Dataset.from_pandas(train_df)\n",
    "labeled_dataset = labeled_dataset.remove_columns(['__index_level_0__'])\n",
    "labeled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926dedb-4133-4d1d-b3df-218c10a855fb",
   "metadata": {},
   "source": [
    "### Read labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b2daf5-21ae-4ac5-bf03-932d24aed5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Dataset/Training/Supervised/labels.txt\", \"r\") as f:\n",
    "    labels = [el.strip() for el in f.readlines() if el.strip()!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9f343bd-998e-40d9-b5a2-d207055c60ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Execution',\n",
       " 'Discovery',\n",
       " 'Persistence',\n",
       " 'Harmless',\n",
       " 'Defense Evasion',\n",
       " 'Impact',\n",
       " 'Other']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de290a-ef2f-483c-898e-8e22268f759a",
   "metadata": {},
   "source": [
    "### Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406ea0a0-4391-4e95-8f07-17f5d909c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Sequence, Value, ClassLabel\n",
    "features = Features(\n",
    "    {\n",
    "        'tokenized_session': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "        'tokens_labels': Sequence(feature = ClassLabel(num_classes=len(labels), names=labels)),\n",
    "        'indexes_statements_context':  Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
    "        'indexes_words_context':  Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
    "        'session_id': Value(dtype='int32', id=None),\n",
    "        'order_id': Value(dtype='int32', id=None),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c61890f-c906-4b02-89ac-3c049f464ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0f338d7eb34d35aec5598c0e0a3c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ast\n",
    "# load the dataset and copy the features > \"tokenized_session\": ast.literal_eval(ex[\"tokenized_session\"]),\n",
    "def process(ex):\n",
    "    return {\"tokenized_session\": ast.literal_eval(ex[\"tokenized_session\"]), \n",
    "            \"tokens_labels\": ast.literal_eval(ex[\"tokens_labels\"]), \n",
    "            \"session_id\": int(ex[\"session_id\"]),\n",
    "            \"order_id\": int(ex[\"order_id\"]),\n",
    "            \"indexes_statements_context\": ast.literal_eval(ex[\"indexes_statements_context\"]),\n",
    "            \"indexes_words_context\": ast.literal_eval(ex[\"indexes_words_context\"])\n",
    "           }\n",
    "labeled_dataset = labeled_dataset.map(process, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c0fc07-0ad2-41fb-9b35-2d4172f9b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokenized_session', 'tokens_labels', 'indexes_statements_context', 'indexes_words_context', 'session_id', 'order_id'],\n",
       "    num_rows: 597\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d907f3c-5541-42a4-b0f6-07e536dd955f",
   "metadata": {},
   "source": [
    "### Let's load a model now\n",
    "#### Tokenizer first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78dd18f9-f456-4ac5-af63-0703d7ef76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84535601-d89d-41cd-9ca8-1166780d5204",
   "metadata": {},
   "source": [
    "### Expand labels to new tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ebd89-80bb-4832-924c-7dc7f2d553a4",
   "metadata": {},
   "source": [
    "#### This version labels all subtokens with the original word's label\n",
    "##### Notice that we also pass the context indexes: those tokens are **not** going to be labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7dec617-6cd2-41ed-9592-ce97948045d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids, indexes_context_words):\n",
    "    new_labels = []\n",
    "    previous_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        elif word_id in indexes_context_words:\n",
    "            # Part of context words I don't want to label\n",
    "            new_labels.append(-100)\n",
    "        elif word_id != previous_word:\n",
    "            # Start of a new word!\n",
    "            previous_word = word_id\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c70321-89d5-4505-890d-3f0f027ee5a7",
   "metadata": {},
   "source": [
    "### Do it for the entire dataset now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1734556f-1436-4a4b-aa30-feaffb8de620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, max_length = tokenizer.model_max_length):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokenized_session\"], truncation=True, is_split_into_words=True, max_length = max_length\n",
    "    )\n",
    "    all_labels = examples[\"tokens_labels\"]\n",
    "    list_context_words = examples[\"indexes_words_context\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        context_words = list_context_words[i]\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids, context_words))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19bfb5a8-0f09-4971-a195-5ded56c547c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c119f6199f6248ea918255f5105be0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = labeled_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    fn_kwargs = {\"max_length\" : tokenizer.model_max_length},\n",
    "    batched=True,\n",
    "    remove_columns=labeled_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430fa762-10d2-4360-af20-729691f152d5",
   "metadata": {},
   "source": [
    "### Now create DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "501d525c-8b12-491b-92a4-f17023b021ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea0932-2e0a-4b30-8287-61c44ea87a7d",
   "metadata": {},
   "source": [
    "### Eventually, define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f031f0b-5ec7-4005-a3c5-935f5f4fbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = labeled_dataset.features[\"tokens_labels\"]\n",
    "label_names = classes.feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b743dac-5986-4ead-90bc-cdc0dc4c6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40643878-5dd6-44b8-87c3-c58626b30873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Execution',\n",
       " 1: 'Discovery',\n",
       " 2: 'Persistence',\n",
       " 3: 'Harmless',\n",
       " 4: 'Defense Evasion',\n",
       " 5: 'Impact',\n",
       " 6: 'Other'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2293706a-5d83-4222-8f19-516ce97f03b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Execution': 0,\n",
       " 'Discovery': 1,\n",
       " 'Persistence': 2,\n",
       " 'Harmless': 3,\n",
       " 'Defense Evasion': 4,\n",
       " 'Impact': 5,\n",
       " 'Other': 6}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08f954-8cc9-49cb-9fef-6c6f9fc1ffce",
   "metadata": {},
   "source": [
    "### Create PyTorch DataLoader \n",
    "#### Necessary, since we want to customize out training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07bd9eb-8933-4183-ad52-cb130566f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8eaae01-8644-4922-a659-a9152f0fa9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tokenized_datasets,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9025f701-dd90-426e-9755-5d46a4886145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"./Finetuned_model/codebert-base_bash_finetuned/tokenizer_pretrained_epochs_5_padded_256/\"\n",
    "config_model = f\"{model_name}/config.json\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    config = config_model\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f0c930c-9678-4637-9097-d0a54a9c89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b908f0-827c-4d17-a350-de93f597cb17",
   "metadata": {},
   "source": [
    "### Define parameters for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cf0e3a6-320d-4b7c-95c5-e05cd9ca9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience_lr_scheduler = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cd94007-8a0f-4dfc-88ba-9432f323299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import get_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "num_train_epochs = stopping_epoch\n",
    "num_update_steps_per_epoch = len(dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = patience_lr_scheduler, verbose = True, min_lr = 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35aff2-78e1-4365-b553-ba85120a4dd8",
   "metadata": {},
   "source": [
    "#### Again, we do not want warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7dcfbe3-7d6c-4a4d-9148-05e98f9e475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.utils.logging.set_verbosity(transformers.utils.logging.ERROR)\n",
    "transformers.utils.logging.enable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ff1a87-edfe-42e1-ad9d-6d9ed2a39542",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46...\r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import copy\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f\"Epoch {epoch}...\", end = \"\\r\")\n",
    "    batch_loss_training = []\n",
    "    # Training\n",
    "    model.train()\n",
    "\n",
    "    for local_batch in dataloader:\n",
    "        batch = local_batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        batch_loss_training.append(outputs.loss.item())\n",
    "        outputs.loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    # Make a scheduler step    \n",
    "    lr_scheduler.step(np.mean(batch_loss_training))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8ac44-f49c-4e86-b726-0366c9185ef4",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e253c4-e05a-4c8f-a2c8-4a0ccc2558ab",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da41f59d-ee33-41d0-8011-58e9b4b404f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"./Trained_Model\", exist_ok = True)\n",
    "model.save_pretrained(f\"./Trained_Model/CodeBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa02ef-5068-43bd-a6c5-3427d532fef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mboffa-cuda]",
   "language": "python",
   "name": "conda-env-mboffa-cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
